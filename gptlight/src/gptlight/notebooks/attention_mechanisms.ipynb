{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7d8cc05a",
   "metadata": {},
   "source": [
    "## Coding attention mechanisms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "314eb10e",
   "metadata": {},
   "source": [
    "At this point, you know how to prepare the input text for training LLMs by splitting\n",
    "text into individual word and subword tokens, which can be encoded into vector rep-\n",
    "resentations, embeddings, for the LLM.\n",
    "Now, we will look at an integral part of the LLM architecture itself, attention\n",
    "mechanisms, as illustrated in figure 3.1. We will largely look at attention mechanisms\n",
    "in isolation and focus on them at a mechanistic level. Then we will code the remaining parts of the LLM surrounding the self-attention mechanism to see it in action and to\n",
    "create a model to generate text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "d977dd98",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69175a09",
   "metadata": {},
   "source": [
    "### A simple self-attention mechanism without trainable weights"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d9ba0d8",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "Consider the following input sentence, which has already been embedded into\n",
    "three-dimensional vectors "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "44712002",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = torch.tensor(\n",
    "    [\n",
    "        [0.43, 0.15, 0.89], # Your(x^1)\n",
    "        [0.55, 0.87, 0.66], # journey(x^2)\n",
    "        [0.57, 0.85, 0.64], # starts(x^3)\n",
    "        [0.22, 0.58, 0.33], # with(x^4)\n",
    "        [0.77, 0.25, 0.10], # one(x^5)\n",
    "        [0.05, 0.80, 0.55]# step(x^6)\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b739fb5",
   "metadata": {},
   "source": [
    "The first step of implementing self-attention is to compute the intermediate values ω,\n",
    "referred to as attention scores."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "242b42da",
   "metadata": {},
   "source": [
    "- attention scores for des second token of the input : journey(x^2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3c8495d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attention weights: tensor([0.1455, 0.2278, 0.2249, 0.1285, 0.1077, 0.1656])\n"
     ]
    }
   ],
   "source": [
    "query2 = inputs[1]\n",
    "attn_scores_2 = torch.empty(inputs.shape[0])\n",
    "for i, key in enumerate(inputs):\n",
    "    attn_scores_2[i] = torch.dot(query2, key)\n",
    "\n",
    "attn_weights_2_tmp = attn_scores_2 / attn_scores_2.sum()\n",
    "\n",
    "print(\"Attention weights:\", attn_weights_2_tmp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa9017b7",
   "metadata": {},
   "source": [
    "In practice, it’s more common and advisable to use the softmax function for normalization. This approach is better at managing extreme values and offers more favorable gradient properties during training. The following is a basic implementation of the softmax function for normalizing the attention scores:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8ae92df5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax_naive(x):\n",
    "    return torch.exp(x) / torch.exp(x).sum(dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "00344d61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attention weights: tensor([0.1385, 0.2379, 0.2333, 0.1240, 0.1082, 0.1581])\n"
     ]
    }
   ],
   "source": [
    "attn_weights_2_naive = softmax_naive(attn_scores_2)\n",
    "print(\"Attention weights:\", attn_weights_2_naive)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b937ac12",
   "metadata": {},
   "source": [
    "Note that this naive softmax implementation (softmax_naive) may encounter\n",
    "numerical instability problems, such as overflow and underflow, when dealing with\n",
    "large or small input values. Therefore, in practice, it’s advisable to use the PyTorch\n",
    "implementation of softmax, which has been extensively optimized for performance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "125b69b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attention weights: tensor([0.1385, 0.2379, 0.2333, 0.1240, 0.1082, 0.1581])\n"
     ]
    }
   ],
   "source": [
    "attn_weights_2 = torch.softmax(attn_scores_2, dim=0)\n",
    "print(\"Attention weights:\", attn_weights_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44e9bf25",
   "metadata": {},
   "source": [
    "Now that we have computed the normalized attention weights, we are ready for the\n",
    "final step : calculating the context vector z(2) by multiplying the\n",
    "embedded input tokens, x(i), with the corresponding attention weights and then sum-\n",
    "ming the resulting vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "468d796b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.4419, 0.6515, 0.5683])\n"
     ]
    }
   ],
   "source": [
    "query = inputs[1]\n",
    "context_vec_2 = torch.zeros(query.shape)\n",
    "for i,x_i in enumerate(inputs):\n",
    "    context_vec_2 += attn_weights_2[i]*x_i\n",
    "print(context_vec_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "216660d9",
   "metadata": {},
   "source": [
    "- Computing attention weights for all input tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "6a509438",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.9995, 0.9544, 0.9422, 0.4753, 0.4576, 0.6310],\n",
      "        [0.9544, 1.4950, 1.4754, 0.8434, 0.7070, 1.0865],\n",
      "        [0.9422, 1.4754, 1.4570, 0.8296, 0.7154, 1.0605],\n",
      "        [0.4753, 0.8434, 0.8296, 0.4937, 0.3474, 0.6565],\n",
      "        [0.4576, 0.7070, 0.7154, 0.3474, 0.6654, 0.2935],\n",
      "        [0.6310, 1.0865, 1.0605, 0.6565, 0.2935, 0.9450]])\n"
     ]
    }
   ],
   "source": [
    "attn_scores = torch.zeros(inputs.shape[0], inputs.shape[0])\n",
    "for i, x_i in enumerate(inputs):\n",
    "    for j, x_j in enumerate(inputs):\n",
    "        attn_scores[i, j] = torch.dot(x_i, x_j)\n",
    "print(attn_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08fb537f",
   "metadata": {},
   "source": [
    "When computing the preceding attention score tensor, we used for loops in\n",
    "Python. However, for loops are generally slow, and we can achieve the same results\n",
    "using matrix multiplication:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "45e8349c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.9995, 0.9544, 0.9422, 0.4753, 0.4576, 0.6310],\n",
      "        [0.9544, 1.4950, 1.4754, 0.8434, 0.7070, 1.0865],\n",
      "        [0.9422, 1.4754, 1.4570, 0.8296, 0.7154, 1.0605],\n",
      "        [0.4753, 0.8434, 0.8296, 0.4937, 0.3474, 0.6565],\n",
      "        [0.4576, 0.7070, 0.7154, 0.3474, 0.6654, 0.2935],\n",
      "        [0.6310, 1.0865, 1.0605, 0.6565, 0.2935, 0.9450]])\n"
     ]
    }
   ],
   "source": [
    "attn_scores = inputs@inputs.T\n",
    "print(attn_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfd100e9",
   "metadata": {},
   "source": [
    "we can now normalize each row with softmax so that the values in each row sum to 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "f957ac51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.2098, 0.2006, 0.1981, 0.1242, 0.1220, 0.1452],\n",
      "        [0.1385, 0.2379, 0.2333, 0.1240, 0.1082, 0.1581],\n",
      "        [0.1390, 0.2369, 0.2326, 0.1242, 0.1108, 0.1565],\n",
      "        [0.1435, 0.2074, 0.2046, 0.1462, 0.1263, 0.1720],\n",
      "        [0.1526, 0.1958, 0.1975, 0.1367, 0.1879, 0.1295],\n",
      "        [0.1385, 0.2184, 0.2128, 0.1420, 0.0988, 0.1896]])\n"
     ]
    }
   ],
   "source": [
    "attn_weights = torch.softmax(attn_scores, dim=-1)\n",
    "print(attn_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a4684fa",
   "metadata": {},
   "source": [
    "In the third and final step, we use these attention weights to compute all\n",
    "context vectors via matrix multiplication:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "0e7765cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.4421, 0.5931, 0.5790],\n",
      "        [0.4419, 0.6515, 0.5683],\n",
      "        [0.4431, 0.6496, 0.5671],\n",
      "        [0.4304, 0.6298, 0.5510],\n",
      "        [0.4671, 0.5910, 0.5266],\n",
      "        [0.4177, 0.6503, 0.5645]])\n"
     ]
    }
   ],
   "source": [
    "context_vecs = attn_weights@inputs\n",
    "print(context_vecs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "085d004a",
   "metadata": {},
   "source": [
    "### Implementing self-attention with trainable weights"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77c13d5c",
   "metadata": {},
   "source": [
    "Our next step will be to implement the self-attention mechanism used in the original transformer architecture, the GPT models, and most other popular LLMs. This self-attention mechanism is also called scaled dot-product attention.\n",
    "\n",
    "The most notable difference is the introduction of weight matrices that are\n",
    "updated during model training. These trainable weight matrices are crucial so that\n",
    "the model (specifically, the attention module inside the model) can learn to produce\n",
    "“good” context vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41b4e87e",
   "metadata": {},
   "source": [
    "- Computing the attention weights step by step\n",
    "  \n",
    "  We will implement the self-attention mechanism step by step by introducing the\n",
    "three trainable weight matrices W_q, W_k , and W_v. These three matrices are used to\n",
    "project the embedded input tokens, x(i), into query, key, and value vectors, respec-\n",
    "tively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26e28542",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_2 = inputs[1]\n",
    "d_in = inputs.shape[1]\n",
    "d_out = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "185fd61d",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(123)\n",
    "W_query = torch.nn.Parameter(torch.rand(d_in, d_out), requires_grad=False)\n",
    "W_key = torch.nn.Parameter(torch.rand(d_in, d_out), requires_grad=False)\n",
    "W_value = torch.nn.Parameter(torch.rand(d_in, d_out), requires_grad=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "6ca8f5bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.4306, 1.4551])\n"
     ]
    }
   ],
   "source": [
    "query_2 = x_2@W_query\n",
    "key_2 = x_2@W_key\n",
    "value_2 = x_2@W_value\n",
    "print(query_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc660662",
   "metadata": {},
   "source": [
    "The output for the query results in a two-dimensional vector since we set the number\n",
    "of columns of the corresponding weight matrix, via d_out, to 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d0b2963",
   "metadata": {},
   "source": [
    "Even though our temporary goal is only to compute the one context vector, z(2), we still\n",
    "require the key and value vectors for all input elements as they are involved in com-\n",
    "puting the attention weights with respect to the query q (2).\n",
    "We can obtain all keys and values via matrix multiplication:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06646780",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "keys.shape: torch.Size([6, 2])\n",
      "values.shape: torch.Size([6, 2])\n"
     ]
    }
   ],
   "source": [
    "keys = inputs@W_key\n",
    "values = inputs@W_value\n",
    "print(\"keys.shape:\", keys.shape)\n",
    "print(\"values.shape:\", values.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b60b604b",
   "metadata": {},
   "source": [
    "The second step is to compute the attention scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "66764fe6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1.2705, 1.8524, 1.8111, 1.0795, 0.5577, 1.5440])\n"
     ]
    }
   ],
   "source": [
    "attn_scores_2 = query_2@keys.T\n",
    "print(attn_scores_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e033b37",
   "metadata": {},
   "source": [
    "Now, we want to go from the attention scores to the attention weights. We compute the attention weights by scaling the attention scores and\n",
    "using the softmax function. However, now we scale the attention scores by dividing\n",
    "them by the square root of the embedding dimension of the keys "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "6c79d8c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.1623, 0.1877, 0.1858, 0.1547, 0.1358, 0.1738])\n"
     ]
    }
   ],
   "source": [
    "d_k = keys.shape[-1]\n",
    "attn_weights_2 = torch.softmax(attn_scores_2/d_k**2, dim=-1)\n",
    "print(attn_weights_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d999a998",
   "metadata": {},
   "source": [
    "Now we can use matrix multiplication between `attn_weights_2` and `values` to obtain the output context vecteur for the token `x^2`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "b9414197",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.2896, 0.7811])\n"
     ]
    }
   ],
   "source": [
    "context_vec_2 = attn_weights_2@values\n",
    "print(context_vec_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7331e3f3",
   "metadata": {},
   "source": [
    "At this point, we have gone through a lot of steps to compute the self-attention out-\n",
    "puts. We did so mainly for illustration purposes so we could go through one step at a\n",
    "time. In practice, with the LLM implementation in the next chapter in mind, it is\n",
    "helpful to organize this code into a Python class, as shown in the following listing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "bc38d492",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SelfAttentionV1(nn.Module):\n",
    "    \n",
    "    def __init__(self, d_in:int, d_out:int):\n",
    "        super().__init__()\n",
    "        self.W_query = nn.Parameter(torch.rand(d_in, d_out))\n",
    "        self.W_key = nn.Parameter(torch.rand(d_in, d_out))\n",
    "        self.W_value = nn.Parameter(torch.rand(d_in, d_out))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        keys = x@self.W_key\n",
    "        queries = x@self.W_key\n",
    "        values = x@self.W_value\n",
    "        attn_scores = queries@keys.T\n",
    "        attn_weights = torch.softmax(attn_scores/keys.shape[-1]**0.5, dim=-1)\n",
    "        context_vecs = attn_weights@values\n",
    "        return context_vecs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "180b3ecc",
   "metadata": {},
   "source": [
    "We can use this class as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "0c5e3a75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.2948, 0.7944],\n",
      "        [0.3013, 0.8099],\n",
      "        [0.3009, 0.8089],\n",
      "        [0.2927, 0.7888],\n",
      "        [0.2866, 0.7737],\n",
      "        [0.2979, 0.8016]], grad_fn=<MmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "sa_v1 = SelfAttentionV1(d_in, d_out)\n",
    "print(sa_v1(inputs))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3523a512",
   "metadata": {},
   "source": [
    "We can improve the SelfAttentionV1 implementation further by utilizing\n",
    "PyTorch’s nn.Linear layers, which effectively perform matrix multiplication when\n",
    "the bias units are disabled. Additionally, a significant advantage of using nn.Linear instead of manually implementing nn.Parameter(torch.rand(...)) is that nn.Linear\n",
    "has an optimized weight initialization scheme, contributing to more stable and\n",
    "effective model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "116ec7e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SelfAttentionV2(nn.Module):\n",
    "    \n",
    "    def __init__(self, d_in:int, d_out:int, qkv_bias:bool=False):\n",
    "        super().__init__()\n",
    "        self.W_query = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_key = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_value = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        keys = self.W_key(x)\n",
    "        queries = self.W_key(x)\n",
    "        values = self.W_value(x)\n",
    "        attn_scores = queries@keys.T\n",
    "        attn_weights = torch.softmax(attn_scores/keys.shape[-1]**0.5, dim=-1)\n",
    "        context_vecs = attn_weights@values\n",
    "        return context_vecs\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53e38e0a",
   "metadata": {},
   "source": [
    "You can use the SelfAttentionV2 similar to SelfAttentionV1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "e7a3d0bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.0793,  0.0640],\n",
      "        [-0.0829,  0.0591],\n",
      "        [-0.0825,  0.0596],\n",
      "        [-0.0815,  0.0608],\n",
      "        [-0.0746,  0.0700],\n",
      "        [-0.0849,  0.0562]], grad_fn=<MmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(789)\n",
    "sa_v2 = SelfAttentionV2(d_in, d_out)\n",
    "print(sa_v2(inputs))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6577f944",
   "metadata": {},
   "source": [
    "To check that both\n",
    "implementations, SelfAttention_v1 and SelfAttention_v2 , are otherwise simi-\n",
    "lar, we can transfer the weight matrices from a SelfAttention_v2 object to a Self-\n",
    "Attention_v1, such that both objects then produce the same results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "f9aebc2a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.0793,  0.0640],\n",
       "        [-0.0829,  0.0591],\n",
       "        [-0.0825,  0.0596],\n",
       "        [-0.0815,  0.0608],\n",
       "        [-0.0746,  0.0700],\n",
       "        [-0.0849,  0.0562]], grad_fn=<MmBackward0>)"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sa_v1.W_query = torch.nn.Parameter(sa_v2.W_query.weight.T)\n",
    "sa_v1.W_key = torch.nn.Parameter(sa_v2.W_key.weight.T)\n",
    "sa_v1.W_value = torch.nn.Parameter(sa_v2.W_value.weight.T)\n",
    "sa_v1(inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab18a076",
   "metadata": {},
   "source": [
    "### Hiding future words with causal attention"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93ac70e0",
   "metadata": {},
   "source": [
    " Causal attention, also known as masked attention, is a specialized form of self-\n",
    "attention. It restricts a model to only consider previous and current inputs in a sequence\n",
    "when processing any given token when computing attention scores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7a87264",
   "metadata": {},
   "source": [
    "#### Applying a causal attention mask"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be9b35a9",
   "metadata": {},
   "source": [
    "One way to obtain the masked attention weight matrix in causal attention is to apply the\n",
    "softmax function to the attention scores, zeroing out the elements above the diagonal and normalizing\n",
    "the resulting matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "4fcb8ec6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.1921, 0.1646, 0.1652, 0.1550, 0.1721, 0.1510],\n",
       "        [0.2041, 0.1659, 0.1662, 0.1496, 0.1665, 0.1477],\n",
       "        [0.2036, 0.1659, 0.1662, 0.1498, 0.1664, 0.1480],\n",
       "        [0.1869, 0.1667, 0.1668, 0.1571, 0.1661, 0.1564],\n",
       "        [0.1830, 0.1669, 0.1670, 0.1588, 0.1658, 0.1585],\n",
       "        [0.1935, 0.1663, 0.1666, 0.1542, 0.1666, 0.1529]],\n",
       "       grad_fn=<SoftmaxBackward0>)"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "queries = sa_v2.W_query(inputs)\n",
    "keys = sa_v2.W_key(inputs)\n",
    "attn_scores = queries@keys.T\n",
    "attn_weights = torch.softmax(attn_scores/keys.shape[-1]**0.5, dim=-1)\n",
    "attn_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abc9c8b8",
   "metadata": {},
   "source": [
    "We can implement the second step using PyTorch’s tril function to create a mask\n",
    "where the values above the diagonal are zero"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "abea74ee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.1921, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.2041, 0.1659, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.2036, 0.1659, 0.1662, 0.0000, 0.0000, 0.0000],\n",
       "        [0.1869, 0.1667, 0.1668, 0.1571, 0.0000, 0.0000],\n",
       "        [0.1830, 0.1669, 0.1670, 0.1588, 0.1658, 0.0000],\n",
       "        [0.1935, 0.1663, 0.1666, 0.1542, 0.1666, 0.1529]],\n",
       "       grad_fn=<MulBackward0>)"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "context_lengt = attn_scores.shape[0]\n",
    "mask_simple = torch.tril(torch.ones(context_lengt, context_lengt))\n",
    "masked_simple = attn_weights*mask_simple\n",
    "masked_simple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "1a5d6659",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.5517, 0.4483, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.3800, 0.3097, 0.3103, 0.0000, 0.0000, 0.0000],\n",
       "        [0.2758, 0.2460, 0.2462, 0.2319, 0.0000, 0.0000],\n",
       "        [0.2175, 0.1983, 0.1984, 0.1888, 0.1971, 0.0000],\n",
       "        [0.1935, 0.1663, 0.1666, 0.1542, 0.1666, 0.1529]],\n",
       "       grad_fn=<DivBackward0>)"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "masked_simple_norm = masked_simple/masked_simple.sum(dim=-1, keepdim=True)\n",
    "masked_simple_norm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da1afc47",
   "metadata": {},
   "source": [
    "When we apply a mask and then renormalize the attention weights, it might initially\n",
    "appear that information from future tokens (which we intend to mask) could still influ-\n",
    "ence the current token because their values are part of the softmax calculation. How-\n",
    "ever, the key insight is that when we renormalize the attention weights after masking what we’re essentially doing is recalculating the softmax over a smaller subset (since\n",
    "masked positions don’t contribute to the softmax value)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81431d97",
   "metadata": {},
   "source": [
    "A more efficient way to obtain the masked attention weight matrix in\n",
    "causal attention is to mask the attention scores with negative infinity values before\n",
    "applying the softmax function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "42cb25d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.2899,   -inf,   -inf,   -inf,   -inf,   -inf],\n",
       "        [0.4656, 0.1723,   -inf,   -inf,   -inf,   -inf],\n",
       "        [0.4594, 0.1703, 0.1731,   -inf,   -inf,   -inf],\n",
       "        [0.2642, 0.1024, 0.1036, 0.0186,   -inf,   -inf],\n",
       "        [0.2183, 0.0874, 0.0882, 0.0177, 0.0786,   -inf],\n",
       "        [0.3408, 0.1270, 0.1290, 0.0198, 0.1290, 0.0078]],\n",
       "       grad_fn=<MaskedFillBackward0>)"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask = torch.triu(torch.ones(context_lengt, context_lengt), diagonal=1)\n",
    "masked = attn_scores.masked_fill(mask.bool(), -torch.inf)\n",
    "masked"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "5078b94c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.5366, 0.4634, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.3660, 0.3168, 0.3172, 0.0000, 0.0000, 0.0000],\n",
       "        [0.2681, 0.2473, 0.2474, 0.2371, 0.0000, 0.0000],\n",
       "        [0.2123, 0.1988, 0.1989, 0.1920, 0.1980, 0.0000],\n",
       "        [0.1853, 0.1665, 0.1667, 0.1578, 0.1667, 0.1569]],\n",
       "       grad_fn=<SoftmaxBackward0>)"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attn_weights = torch.softmax(masked/keys.shape[-1], dim=-1)\n",
    "attn_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c156bec5",
   "metadata": {},
   "source": [
    "#### Masking additional attention weights with dropout"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
