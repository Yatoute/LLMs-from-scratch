{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7d8cc05a",
   "metadata": {},
   "source": [
    "## Coding attention mechanisms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "314eb10e",
   "metadata": {},
   "source": [
    "At this point, you know how to prepare the input text for training LLMs by splitting\n",
    "text into individual word and subword tokens, which can be encoded into vector rep-\n",
    "resentations, embeddings, for the LLM.\n",
    "Now, we will look at an integral part of the LLM architecture itself, attention\n",
    "mechanisms, as illustrated in figure 3.1. We will largely look at attention mechanisms\n",
    "in isolation and focus on them at a mechanistic level. Then we will code the remaining parts of the LLM surrounding the self-attention mechanism to see it in action and to\n",
    "create a model to generate text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "d977dd98",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69175a09",
   "metadata": {},
   "source": [
    "### A simple self-attention mechanism without trainable weights"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d9ba0d8",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "Consider the following input sentence, which has already been embedded into\n",
    "three-dimensional vectors "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "44712002",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = torch.tensor(\n",
    "    [\n",
    "        [0.43, 0.15, 0.89], # Your(x^1)\n",
    "        [0.55, 0.87, 0.66], # journey(x^2)\n",
    "        [0.57, 0.85, 0.64], # starts(x^3)\n",
    "        [0.22, 0.58, 0.33], # with(x^4)\n",
    "        [0.77, 0.25, 0.10], # one(x^5)\n",
    "        [0.05, 0.80, 0.55]# step(x^6)\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b739fb5",
   "metadata": {},
   "source": [
    "The first step of implementing self-attention is to compute the intermediate values ω,\n",
    "referred to as attention scores."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "242b42da",
   "metadata": {},
   "source": [
    "- attention scores for des second token of the input : journey(x^2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3c8495d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attention weights: tensor([0.1455, 0.2278, 0.2249, 0.1285, 0.1077, 0.1656])\n"
     ]
    }
   ],
   "source": [
    "query2 = inputs[1]\n",
    "attn_scores_2 = torch.empty(inputs.shape[0])\n",
    "for i, key in enumerate(inputs):\n",
    "    attn_scores_2[i] = torch.dot(query2, key)\n",
    "\n",
    "attn_weights_2_tmp = attn_scores_2 / attn_scores_2.sum()\n",
    "\n",
    "print(\"Attention weights:\", attn_weights_2_tmp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa9017b7",
   "metadata": {},
   "source": [
    "In practice, it’s more common and advisable to use the softmax function for normalization. This approach is better at managing extreme values and offers more favorable gradient properties during training. The following is a basic implementation of the softmax function for normalizing the attention scores:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8ae92df5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax_naive(x):\n",
    "    return torch.exp(x) / torch.exp(x).sum(dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "00344d61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attention weights: tensor([0.1385, 0.2379, 0.2333, 0.1240, 0.1082, 0.1581])\n"
     ]
    }
   ],
   "source": [
    "attn_weights_2_naive = softmax_naive(attn_scores_2)\n",
    "print(\"Attention weights:\", attn_weights_2_naive)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b937ac12",
   "metadata": {},
   "source": [
    "Note that this naive softmax implementation (softmax_naive) may encounter\n",
    "numerical instability problems, such as overflow and underflow, when dealing with\n",
    "large or small input values. Therefore, in practice, it’s advisable to use the PyTorch\n",
    "implementation of softmax, which has been extensively optimized for performance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "125b69b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attention weights: tensor([0.1385, 0.2379, 0.2333, 0.1240, 0.1082, 0.1581])\n"
     ]
    }
   ],
   "source": [
    "attn_weights_2 = torch.softmax(attn_scores_2, dim=0)\n",
    "print(\"Attention weights:\", attn_weights_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44e9bf25",
   "metadata": {},
   "source": [
    "Now that we have computed the normalized attention weights, we are ready for the\n",
    "final step : calculating the context vector z(2) by multiplying the\n",
    "embedded input tokens, x(i), with the corresponding attention weights and then sum-\n",
    "ming the resulting vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "468d796b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.4419, 0.6515, 0.5683])\n"
     ]
    }
   ],
   "source": [
    "query = inputs[1]\n",
    "context_vec_2 = torch.zeros(query.shape)\n",
    "for i,x_i in enumerate(inputs):\n",
    "    context_vec_2 += attn_weights_2[i]*x_i\n",
    "print(context_vec_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "216660d9",
   "metadata": {},
   "source": [
    "- Computing attention weights for all input tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "6a509438",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.9995, 0.9544, 0.9422, 0.4753, 0.4576, 0.6310],\n",
      "        [0.9544, 1.4950, 1.4754, 0.8434, 0.7070, 1.0865],\n",
      "        [0.9422, 1.4754, 1.4570, 0.8296, 0.7154, 1.0605],\n",
      "        [0.4753, 0.8434, 0.8296, 0.4937, 0.3474, 0.6565],\n",
      "        [0.4576, 0.7070, 0.7154, 0.3474, 0.6654, 0.2935],\n",
      "        [0.6310, 1.0865, 1.0605, 0.6565, 0.2935, 0.9450]])\n"
     ]
    }
   ],
   "source": [
    "attn_scores = torch.zeros(inputs.shape[0], inputs.shape[0])\n",
    "for i, x_i in enumerate(inputs):\n",
    "    for j, x_j in enumerate(inputs):\n",
    "        attn_scores[i, j] = torch.dot(x_i, x_j)\n",
    "print(attn_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08fb537f",
   "metadata": {},
   "source": [
    "When computing the preceding attention score tensor, we used for loops in\n",
    "Python. However, for loops are generally slow, and we can achieve the same results\n",
    "using matrix multiplication:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "45e8349c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.9995, 0.9544, 0.9422, 0.4753, 0.4576, 0.6310],\n",
      "        [0.9544, 1.4950, 1.4754, 0.8434, 0.7070, 1.0865],\n",
      "        [0.9422, 1.4754, 1.4570, 0.8296, 0.7154, 1.0605],\n",
      "        [0.4753, 0.8434, 0.8296, 0.4937, 0.3474, 0.6565],\n",
      "        [0.4576, 0.7070, 0.7154, 0.3474, 0.6654, 0.2935],\n",
      "        [0.6310, 1.0865, 1.0605, 0.6565, 0.2935, 0.9450]])\n"
     ]
    }
   ],
   "source": [
    "attn_scores = inputs@inputs.T\n",
    "print(attn_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfd100e9",
   "metadata": {},
   "source": [
    "we can now normalize each row with softmax so that the values in each row sum to 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "f957ac51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.2098, 0.2006, 0.1981, 0.1242, 0.1220, 0.1452],\n",
      "        [0.1385, 0.2379, 0.2333, 0.1240, 0.1082, 0.1581],\n",
      "        [0.1390, 0.2369, 0.2326, 0.1242, 0.1108, 0.1565],\n",
      "        [0.1435, 0.2074, 0.2046, 0.1462, 0.1263, 0.1720],\n",
      "        [0.1526, 0.1958, 0.1975, 0.1367, 0.1879, 0.1295],\n",
      "        [0.1385, 0.2184, 0.2128, 0.1420, 0.0988, 0.1896]])\n"
     ]
    }
   ],
   "source": [
    "attn_weights = torch.softmax(attn_scores, dim=-1)\n",
    "print(attn_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a4684fa",
   "metadata": {},
   "source": [
    "In the third and final step, we use these attention weights to compute all\n",
    "context vectors via matrix multiplication:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "0e7765cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.4421, 0.5931, 0.5790],\n",
      "        [0.4419, 0.6515, 0.5683],\n",
      "        [0.4431, 0.6496, 0.5671],\n",
      "        [0.4304, 0.6298, 0.5510],\n",
      "        [0.4671, 0.5910, 0.5266],\n",
      "        [0.4177, 0.6503, 0.5645]])\n"
     ]
    }
   ],
   "source": [
    "context_vecs = attn_weights@inputs\n",
    "print(context_vecs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "085d004a",
   "metadata": {},
   "source": [
    "### Implementing self-attention with trainable weights"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77c13d5c",
   "metadata": {},
   "source": [
    "Our next step will be to implement the self-attention mechanism used in the original transformer architecture, the GPT models, and most other popular LLMs. This self-attention mechanism is also called scaled dot-product attention.\n",
    "\n",
    "The most notable difference is the introduction of weight matrices that are\n",
    "updated during model training. These trainable weight matrices are crucial so that\n",
    "the model (specifically, the attention module inside the model) can learn to produce\n",
    "“good” context vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41b4e87e",
   "metadata": {},
   "source": [
    "- Computing the attention weights step by step\n",
    "  \n",
    "  We will implement the self-attention mechanism step by step by introducing the\n",
    "three trainable weight matrices W_q, W_k , and W_v. These three matrices are used to\n",
    "project the embedded input tokens, x(i), into query, key, and value vectors, respec-\n",
    "tively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26e28542",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_2 = inputs[1]\n",
    "d_in = inputs.shape[1]\n",
    "d_out = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "185fd61d",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(123)\n",
    "W_query = torch.nn.Parameter(torch.rand(d_in, d_out), requires_grad=False)\n",
    "W_key = torch.nn.Parameter(torch.rand(d_in, d_out), requires_grad=False)\n",
    "W_value = torch.nn.Parameter(torch.rand(d_in, d_out), requires_grad=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "6ca8f5bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.4306, 1.4551])\n"
     ]
    }
   ],
   "source": [
    "query_2 = x_2@W_query\n",
    "key_2 = x_2@W_key\n",
    "value_2 = x_2@W_value\n",
    "print(query_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc660662",
   "metadata": {},
   "source": [
    "The output for the query results in a two-dimensional vector since we set the number\n",
    "of columns of the corresponding weight matrix, via d_out, to 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d0b2963",
   "metadata": {},
   "source": [
    "Even though our temporary goal is only to compute the one context vector, z(2), we still\n",
    "require the key and value vectors for all input elements as they are involved in com-\n",
    "puting the attention weights with respect to the query q (2).\n",
    "We can obtain all keys and values via matrix multiplication:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06646780",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "keys.shape: torch.Size([6, 2])\n",
      "values.shape: torch.Size([6, 2])\n"
     ]
    }
   ],
   "source": [
    "keys = inputs@W_key\n",
    "values = inputs@W_value\n",
    "print(\"keys.shape:\", keys.shape)\n",
    "print(\"values.shape:\", values.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b60b604b",
   "metadata": {},
   "source": [
    "The second step is to compute the attention scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "66764fe6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1.2705, 1.8524, 1.8111, 1.0795, 0.5577, 1.5440])\n"
     ]
    }
   ],
   "source": [
    "attn_scores_2 = query_2@keys.T\n",
    "print(attn_scores_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e033b37",
   "metadata": {},
   "source": [
    "Now, we want to go from the attention scores to the attention weights. We compute the attention weights by scaling the attention scores and\n",
    "using the softmax function. However, now we scale the attention scores by dividing\n",
    "them by the square root of the embedding dimension of the keys "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "6c79d8c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.1623, 0.1877, 0.1858, 0.1547, 0.1358, 0.1738])\n"
     ]
    }
   ],
   "source": [
    "d_k = keys.shape[-1]\n",
    "attn_weights_2 = torch.softmax(attn_scores_2/d_k**2, dim=-1)\n",
    "print(attn_weights_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d999a998",
   "metadata": {},
   "source": [
    "Now we can use matrix multiplication between `attn_weights_2` and `values` to obtain the output context vecteur for the token `x^2`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "b9414197",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.2896, 0.7811])\n"
     ]
    }
   ],
   "source": [
    "context_vec_2 = attn_weights_2@values\n",
    "print(context_vec_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7331e3f3",
   "metadata": {},
   "source": [
    "At this point, we have gone through a lot of steps to compute the self-attention out-\n",
    "puts. We did so mainly for illustration purposes so we could go through one step at a\n",
    "time. In practice, with the LLM implementation in the next chapter in mind, it is\n",
    "helpful to organize this code into a Python class, as shown in the following listing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "bc38d492",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SelfAttentionV1(nn.Module):\n",
    "    \n",
    "    def __init__(self, d_in:int, d_out:int):\n",
    "        super().__init__()\n",
    "        self.W_query = nn.Parameter(torch.rand(d_in, d_out))\n",
    "        self.W_key = nn.Parameter(torch.rand(d_in, d_out))\n",
    "        self.W_value = nn.Parameter(torch.rand(d_in, d_out))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        keys = x@self.W_key\n",
    "        queries = x@self.W_key\n",
    "        values = x@self.W_value\n",
    "        attn_scores = queries@keys.T\n",
    "        attn_weights = torch.softmax(attn_scores/keys.shape[-1]**0.5, dim=-1)\n",
    "        context_vecs = attn_weights@values\n",
    "        return context_vecs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "180b3ecc",
   "metadata": {},
   "source": [
    "We can use this class as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "0c5e3a75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.2948, 0.7944],\n",
      "        [0.3013, 0.8099],\n",
      "        [0.3009, 0.8089],\n",
      "        [0.2927, 0.7888],\n",
      "        [0.2866, 0.7737],\n",
      "        [0.2979, 0.8016]], grad_fn=<MmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "sa_v1 = SelfAttentionV1(d_in, d_out)\n",
    "print(sa_v1(inputs))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3523a512",
   "metadata": {},
   "source": [
    "We can improve the SelfAttentionV1 implementation further by utilizing\n",
    "PyTorch’s nn.Linear layers, which effectively perform matrix multiplication when\n",
    "the bias units are disabled. Additionally, a significant advantage of using nn.Linear instead of manually implementing nn.Parameter(torch.rand(...)) is that nn.Linear\n",
    "has an optimized weight initialization scheme, contributing to more stable and\n",
    "effective model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "116ec7e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SelfAttentionV2(nn.Module):\n",
    "    \n",
    "    def __init__(self, d_in:int, d_out:int, qkv_bias:bool=False):\n",
    "        super().__init__()\n",
    "        self.W_query = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_key = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_value = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        keys = self.W_key(x)\n",
    "        queries = self.W_key(x)\n",
    "        values = self.W_value(x)\n",
    "        attn_scores = queries@keys.T\n",
    "        attn_weights = torch.softmax(attn_scores/keys.shape[-1]**0.5, dim=-1)\n",
    "        context_vecs = attn_weights@values\n",
    "        return context_vecs\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53e38e0a",
   "metadata": {},
   "source": [
    "You can use the SelfAttentionV2 similar to SelfAttentionV1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "e7a3d0bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.0793,  0.0640],\n",
      "        [-0.0829,  0.0591],\n",
      "        [-0.0825,  0.0596],\n",
      "        [-0.0815,  0.0608],\n",
      "        [-0.0746,  0.0700],\n",
      "        [-0.0849,  0.0562]], grad_fn=<MmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(789)\n",
    "sa_v2 = SelfAttentionV2(d_in, d_out)\n",
    "print(sa_v2(inputs))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6577f944",
   "metadata": {},
   "source": [
    "To check that both\n",
    "implementations, SelfAttention_v1 and SelfAttention_v2 , are otherwise simi-\n",
    "lar, we can transfer the weight matrices from a SelfAttention_v2 object to a Self-\n",
    "Attention_v1, such that both objects then produce the same results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "f9aebc2a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.0793,  0.0640],\n",
       "        [-0.0829,  0.0591],\n",
       "        [-0.0825,  0.0596],\n",
       "        [-0.0815,  0.0608],\n",
       "        [-0.0746,  0.0700],\n",
       "        [-0.0849,  0.0562]], grad_fn=<MmBackward0>)"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sa_v1.W_query = torch.nn.Parameter(sa_v2.W_query.weight.T)\n",
    "sa_v1.W_key = torch.nn.Parameter(sa_v2.W_key.weight.T)\n",
    "sa_v1.W_value = torch.nn.Parameter(sa_v2.W_value.weight.T)\n",
    "sa_v1(inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab18a076",
   "metadata": {},
   "source": [
    "### Hiding future words with causal attention"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93ac70e0",
   "metadata": {},
   "source": [
    " Causal attention, also known as masked attention, is a specialized form of self-\n",
    "attention. It restricts a model to only consider previous and current inputs in a sequence\n",
    "when processing any given token when computing attention scores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7a87264",
   "metadata": {},
   "source": [
    "#### Applying a causal attention mask"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be9b35a9",
   "metadata": {},
   "source": [
    "One way to obtain the masked attention weight matrix in causal attention is to apply the\n",
    "softmax function to the attention scores, zeroing out the elements above the diagonal and normalizing\n",
    "the resulting matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "4fcb8ec6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.1921, 0.1646, 0.1652, 0.1550, 0.1721, 0.1510],\n",
       "        [0.2041, 0.1659, 0.1662, 0.1496, 0.1665, 0.1477],\n",
       "        [0.2036, 0.1659, 0.1662, 0.1498, 0.1664, 0.1480],\n",
       "        [0.1869, 0.1667, 0.1668, 0.1571, 0.1661, 0.1564],\n",
       "        [0.1830, 0.1669, 0.1670, 0.1588, 0.1658, 0.1585],\n",
       "        [0.1935, 0.1663, 0.1666, 0.1542, 0.1666, 0.1529]],\n",
       "       grad_fn=<SoftmaxBackward0>)"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "queries = sa_v2.W_query(inputs)\n",
    "keys = sa_v2.W_key(inputs)\n",
    "attn_scores = queries@keys.T\n",
    "attn_weights = torch.softmax(attn_scores/keys.shape[-1]**0.5, dim=-1)\n",
    "attn_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abc9c8b8",
   "metadata": {},
   "source": [
    "We can implement the second step using PyTorch’s tril function to create a mask\n",
    "where the values above the diagonal are zero"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "abea74ee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.1921, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.2041, 0.1659, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.2036, 0.1659, 0.1662, 0.0000, 0.0000, 0.0000],\n",
       "        [0.1869, 0.1667, 0.1668, 0.1571, 0.0000, 0.0000],\n",
       "        [0.1830, 0.1669, 0.1670, 0.1588, 0.1658, 0.0000],\n",
       "        [0.1935, 0.1663, 0.1666, 0.1542, 0.1666, 0.1529]],\n",
       "       grad_fn=<MulBackward0>)"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "context_lengt = attn_scores.shape[0]\n",
    "mask_simple = torch.tril(torch.ones(context_lengt, context_lengt))\n",
    "masked_simple = attn_weights*mask_simple\n",
    "masked_simple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "1a5d6659",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.5517, 0.4483, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.3800, 0.3097, 0.3103, 0.0000, 0.0000, 0.0000],\n",
       "        [0.2758, 0.2460, 0.2462, 0.2319, 0.0000, 0.0000],\n",
       "        [0.2175, 0.1983, 0.1984, 0.1888, 0.1971, 0.0000],\n",
       "        [0.1935, 0.1663, 0.1666, 0.1542, 0.1666, 0.1529]],\n",
       "       grad_fn=<DivBackward0>)"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "masked_simple_norm = masked_simple/masked_simple.sum(dim=-1, keepdim=True)\n",
    "masked_simple_norm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da1afc47",
   "metadata": {},
   "source": [
    "When we apply a mask and then renormalize the attention weights, it might initially\n",
    "appear that information from future tokens (which we intend to mask) could still influ-\n",
    "ence the current token because their values are part of the softmax calculation. How-\n",
    "ever, the key insight is that when we renormalize the attention weights after masking what we’re essentially doing is recalculating the softmax over a smaller subset (since\n",
    "masked positions don’t contribute to the softmax value)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81431d97",
   "metadata": {},
   "source": [
    "A more efficient way to obtain the masked attention weight matrix in\n",
    "causal attention is to mask the attention scores with negative infinity values before\n",
    "applying the softmax function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "42cb25d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.2899,   -inf,   -inf,   -inf,   -inf,   -inf],\n",
       "        [0.4656, 0.1723,   -inf,   -inf,   -inf,   -inf],\n",
       "        [0.4594, 0.1703, 0.1731,   -inf,   -inf,   -inf],\n",
       "        [0.2642, 0.1024, 0.1036, 0.0186,   -inf,   -inf],\n",
       "        [0.2183, 0.0874, 0.0882, 0.0177, 0.0786,   -inf],\n",
       "        [0.3408, 0.1270, 0.1290, 0.0198, 0.1290, 0.0078]],\n",
       "       grad_fn=<MaskedFillBackward0>)"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask = torch.triu(torch.ones(context_lengt, context_lengt), diagonal=1)\n",
    "masked = attn_scores.masked_fill(mask.bool(), -torch.inf)\n",
    "masked"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "5078b94c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.5366, 0.4634, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.3660, 0.3168, 0.3172, 0.0000, 0.0000, 0.0000],\n",
       "        [0.2681, 0.2473, 0.2474, 0.2371, 0.0000, 0.0000],\n",
       "        [0.2123, 0.1988, 0.1989, 0.1920, 0.1980, 0.0000],\n",
       "        [0.1853, 0.1665, 0.1667, 0.1578, 0.1667, 0.1569]],\n",
       "       grad_fn=<SoftmaxBackward0>)"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attn_weights = torch.softmax(masked/keys.shape[-1], dim=-1)\n",
    "attn_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c156bec5",
   "metadata": {},
   "source": [
    "#### Masking additional attention weights with dropout"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "966fa68d",
   "metadata": {},
   "source": [
    "Dropout in deep learning is a technique where randomly selected hidden layer units are ignored during training, effectively “dropping” them out. This method helps prevent overfitting by ensuring that a model does not become overly reliant on any specific set of hidden layer units. It’s important to emphasize that dropout is only used during training and is disabled afterward."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "43715c9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[2., 2., 2., 2., 2., 2.],\n",
      "        [0., 2., 0., 0., 0., 0.],\n",
      "        [0., 0., 2., 0., 2., 0.],\n",
      "        [2., 2., 0., 0., 0., 2.],\n",
      "        [2., 0., 0., 0., 0., 2.],\n",
      "        [0., 2., 0., 0., 0., 0.]])\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "dropout = torch.nn.Dropout(0.5) #dropout 50% of tokens\n",
    "example = torch.ones(6, 6)\n",
    "print(dropout(example))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebf4c06c",
   "metadata": {},
   "source": [
    "When applying dropout to an attention weight matrix with a rate of 50%, half of the\n",
    "elements in the matrix are randomly set to zero. To compensate for the reduction in\n",
    "active elements, the values of the remaining elements in the matrix are scaled up by a\n",
    "factor of 1/0.5 = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf49bd3c",
   "metadata": {},
   "source": [
    "Now let’s apply dropout to the attention weight matrix itself"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "44f8a755",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.5366, 0.4634, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.3660, 0.3168, 0.3172, 0.0000, 0.0000, 0.0000],\n",
       "        [0.2681, 0.2473, 0.2474, 0.2371, 0.0000, 0.0000],\n",
       "        [0.2123, 0.1988, 0.1989, 0.1920, 0.1980, 0.0000],\n",
       "        [0.1853, 0.1665, 0.1667, 0.1578, 0.1667, 0.1569]],\n",
       "       grad_fn=<SoftmaxBackward0>)"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attn_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "886bb021",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[2.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.9268, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.6344, 0.0000, 0.0000, 0.0000],\n",
      "        [0.5362, 0.4946, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.4246, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.3331, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "       grad_fn=<MulBackward0>)\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "print(dropout(attn_weights))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df422a4d",
   "metadata": {},
   "source": [
    "### Implementing a compact causal attention class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "006871d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------------------------------------------------------\n",
    "# CausalAttention : implémentation d'une attention \"causale\" de style GPT\n",
    "#\n",
    "# - Cette couche projette les entrées (x) en trois matrices : requêtes (Q),\n",
    "#   clés (K) et valeurs (V) via trois couches linéaires.\n",
    "#\n",
    "# - Le masque causal (matrice triangulaire supérieure) empêche chaque token\n",
    "#   d'accéder aux tokens futurs : un token t ne peut assister qu'aux positions\n",
    "#   ≤ t. Cela impose la propriété *auto-régressive*, essentielle pour les GPT.\n",
    "#\n",
    "# - Le score d’attention est calculé par Q @ Kᵀ puis normalisé via softmax\n",
    "#   (après l'application du masque).\n",
    "#\n",
    "# - On applique ensuite un dropout sur les poids d’attention pour régulariser.\n",
    "#\n",
    "# - Le vecteur de contexte est obtenu par : AttentionWeights @ V,\n",
    "#   ce qui permet à chaque token d'agréger l'information pertinente des tokens\n",
    "#   précédents.\n",
    "#\n",
    "# Résultat : une couche d’attention correcte pour un modèle de langage\n",
    "# auto-régressif, où chaque position ne dépend que du passé, jamais du futur.\n",
    "\n",
    "\n",
    "# register_buffer() permet de stocker un tenseur dans le module\n",
    "# sans le considérer comme un paramètre entraînable.\n",
    "# Avantages :\n",
    "# - déplacé automatiquement sur CPU/GPU avec model.to(device)\n",
    "# - sauvegardé dans state_dict()\n",
    "# - pas mis à jour par l’optimizer\n",
    "#\n",
    "# Idéal pour les masques causaux, constantes, etc.\n",
    "\n",
    "# -----------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "e3c19b08",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CausalAttention(nn.Module):\n",
    "    \n",
    "    def __init__(self, d_in:int, d_out:int, context_length:int, dropout:float, qkv_bias:bool=False):\n",
    "        super().__init__()\n",
    "        self.W_query = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_key = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_value = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.register_buffer(\n",
    "            'mask',\n",
    "            torch.triu(torch.ones(context_length, context_length), diagonal=1)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x:torch.Tensor):\n",
    "        num_tokens = x.shape[1]\n",
    "        queries = self.W_query(x)\n",
    "        keys = self.W_key(x)\n",
    "        values = self.W_value(x)\n",
    "        attn_scores = queries@keys.transpose(1, 2)\n",
    "        attn_scores.masked_fill_(\n",
    "            self.mask.bool()[:num_tokens, :num_tokens], -torch.inf\n",
    "        )\n",
    "        attn_weights = torch.softmax(attn_scores/keys.shape[-1]**0.5, dim=-1)\n",
    "        attn_weights = self.dropout(attn_weights)\n",
    "        context_vecs = attn_weights@values\n",
    "        return context_vecs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "2d8fdc3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 6, 3])\n"
     ]
    }
   ],
   "source": [
    "batch = torch.stack((inputs, inputs), dim=0)\n",
    "print(batch.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "9a6ad026",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "conttext_vecs: tensor([[[-0.4519,  0.2216],\n",
      "         [-0.5874,  0.0058],\n",
      "         [-0.6300, -0.0632],\n",
      "         [-0.5675, -0.0843],\n",
      "         [-0.5526, -0.0981],\n",
      "         [-0.5299, -0.1081]],\n",
      "\n",
      "        [[-0.4519,  0.2216],\n",
      "         [-0.5874,  0.0058],\n",
      "         [-0.6300, -0.0632],\n",
      "         [-0.5675, -0.0843],\n",
      "         [-0.5526, -0.0981],\n",
      "         [-0.5299, -0.1081]]], grad_fn=<UnsafeViewBackward0>)\n",
      "\n",
      "context_vecs.shape: torch.Size([2, 6, 2])\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "context_length = batch.shape[1]\n",
    "ca = CausalAttention(d_in, d_out, context_length, 0.0)\n",
    "context_vecs = ca(batch)\n",
    "print(\"conttext_vecs:\", context_vecs)\n",
    "print(\"\\ncontext_vecs.shape:\", context_vecs.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8986577f",
   "metadata": {},
   "source": [
    "## Extending single-head attention to multi-head attention"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66fa6511",
   "metadata": {},
   "source": [
    "Our final step will be to extend the previously implemented causal attention class over\n",
    "multiple heads. This is also called multi-head attention.\n",
    "\n",
    "The term “multi-head” refers to dividing the attention mechanism into multiple\n",
    "“heads,” each operating independently. In this context, a single causal attention mod-\n",
    "ule can be considered single-head attention, where there is only one set of attention\n",
    "weights processing the input sequentially."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d6cd2cc",
   "metadata": {},
   "source": [
    "### Stacking multiple single-head attention layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26df12f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttentionWrapper(nn.Module):\n",
    "    \n",
    "    def __init__(self, d_in:int, d_out:int, context_length:int, dropout:int, num_heads:int, qkv_bias:bool=False):\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList(\n",
    "            [\n",
    "                CausalAttention(d_in, d_out, context_length, dropout, qkv_bias) for _ in range(num_heads)\n",
    "            ]\n",
    "        )\n",
    "        \n",
    "    def forward(self, x:torch.Tensor):\n",
    "        \n",
    "        return torch.cat([head(x) for head in self.heads], dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f4abec3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "conttext_vecs: tensor([[[-0.4519,  0.2216,  0.4772,  0.1063,  0.4566,  0.2729],\n",
      "         [-0.5874,  0.0058,  0.5891,  0.3257,  0.5792,  0.3011],\n",
      "         [-0.6300, -0.0632,  0.6202,  0.3860,  0.6249,  0.3102],\n",
      "         [-0.5675, -0.0843,  0.5478,  0.3589,  0.5691,  0.2785],\n",
      "         [-0.5526, -0.0981,  0.5321,  0.3428,  0.5543,  0.2520],\n",
      "         [-0.5299, -0.1081,  0.5077,  0.3493,  0.5337,  0.2499]],\n",
      "\n",
      "        [[-0.4519,  0.2216,  0.4772,  0.1063,  0.4566,  0.2729],\n",
      "         [-0.5874,  0.0058,  0.5891,  0.3257,  0.5792,  0.3011],\n",
      "         [-0.6300, -0.0632,  0.6202,  0.3860,  0.6249,  0.3102],\n",
      "         [-0.5675, -0.0843,  0.5478,  0.3589,  0.5691,  0.2785],\n",
      "         [-0.5526, -0.0981,  0.5321,  0.3428,  0.5543,  0.2520],\n",
      "         [-0.5299, -0.1081,  0.5077,  0.3493,  0.5337,  0.2499]]],\n",
      "       grad_fn=<CatBackward0>)\n",
      "\n",
      "context_vecs.shape: torch.Size([2, 6, 6])\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "mhaw = MultiHeadAttentionWrapper(d_in, d_out, context_length, 0.0, num_heads=3)\n",
    "context_vecs = mhaw(batch)\n",
    "print(\"conttext_vecs:\", context_vecs)\n",
    "print(\"\\ncontext_vecs.shape:\", context_vecs.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78266427",
   "metadata": {},
   "source": [
    "###  Implementing multi-head attention with weight splits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb3ba357",
   "metadata": {},
   "source": [
    "In the MultiHeadAttentionWrapper, multiple heads are implemented by creating\n",
    "a list of CausalAttention objects (self.heads), each representing a separate attention head. The CausalAttention class independently performs the attention mechanism, and the results from each head are concatenated. In contrast, the following\n",
    "MultiHeadAttention class integrates the multi-head functionality within a single class.\n",
    "It splits the input into multiple heads by reshaping the projected query, key, and value\n",
    "tensors and then combines the results from these heads after computing attention."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "id": "44f0cdc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    \n",
    "    def __init__(self, d_in:int, d_out:int, num_heads:int, context_length:int, dropout:float, qkv_bias:bool=False):\n",
    "        super().__init__()\n",
    "        assert (d_out % num_heads == 0), \"d_out must be divisible by num_heads\"\n",
    "        \n",
    "        self.d_out = d_out\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = d_out//num_heads\n",
    "        self.W_query = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_keys = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_value = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.register_buffer(\n",
    "            \"mask\",\n",
    "            torch.triu(torch.ones(context_length, context_length), diagonal=1)\n",
    "        )\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.out_proj = nn.Linear(d_out, d_out)\n",
    "   \n",
    "    def forward(self, x:torch.Tensor):\n",
    "        \n",
    "        num_batchs, num_tokens, d_in = x.shape\n",
    "        \n",
    "        keys = self.W_keys(x) # -> (num_batchs, num_tokens, d_out=num_heads*head_dim)\n",
    "        queries = self.W_query(x) # -> (num_batchs, num_tokens, d_out=num_heads*head_dim)\n",
    "        values = self.W_value(x) # -> (num_batchs, num_tokens, d_out=num_heads*head_dim)\n",
    "        print(f\"keys : {keys}\")\n",
    "        \n",
    "        keys = keys.view(num_batchs, num_tokens, self.num_heads, self.head_dim) # -> (num_batchs, num_tokens, num_heads, head_dim)\n",
    "        queries = queries.view(num_batchs, num_tokens, self.num_heads, self.head_dim)\n",
    "        values= values.view(num_batchs, num_tokens, self.num_heads, self.head_dim)\n",
    "        print(f\"keys : {keys}\")\n",
    "        \n",
    "        keys = keys.transpose(1, 2) # -> (num_batchs, num_heads, num_tokens, head_dim)\n",
    "        queries = queries.transpose(1, 2)\n",
    "        values = values.transpose(1, 2)\n",
    "        print(f\"keys : {keys}\")\n",
    "        \n",
    "        attn_scores = queries@keys.transpose(-2, -1) # -> (num_batchs, num_heads, num_tokens, num_tokens)\n",
    "        attn_scores.masked_fill_(\n",
    "            self.mask.bool()[:num_tokens, :num_tokens], -torch.inf\n",
    "        )\n",
    "        print(f\"attn_scores : {attn_scores}\")\n",
    "        \n",
    "        attn_weights = torch.softmax(attn_scores/keys.shape[-1]**0.5, dim=-1)\n",
    "        attn_weights = self.dropout(attn_weights)\n",
    "        print(f\"Attn_weights : {attn_weights}\")\n",
    "        \n",
    "        print(f\"attn_weights@values : {attn_weights@values}\")\n",
    "        \n",
    "        context_vecs = (attn_weights@values).transpose(1, 2) # -> (num_batchs,  num_tokens, num_heads, head_dim)\n",
    "        \n",
    "        print(f\"context_vecs : {context_vecs}\")\n",
    "        \n",
    "        context_vecs = context_vecs.contiguous().view(num_batchs,  num_tokens, self.d_out) # -> (num_batchs,  num_tokens, d_out = num_heads*head_dim)\n",
    "        print(f\"context_vecs : {context_vecs}\")\n",
    "          \n",
    "        context_vecs = self.out_proj(context_vecs)\n",
    "        \n",
    "        return context_vecs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "id": "49f3a455",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "keys : tensor([[[ 0.0618, -0.0698, -0.1751, -0.0632, -0.6651, -0.5935],\n",
      "         [-0.2447,  0.3075,  0.3275, -0.3468, -0.9567, -0.9076],\n",
      "         [-0.2248,  0.2904,  0.3332, -0.3266, -0.9397, -0.9034],\n",
      "         [-0.2204,  0.2532,  0.2174, -0.2655, -0.5439, -0.4879],\n",
      "         [ 0.1968, -0.0995,  0.3421,  0.1291, -0.3686, -0.5725],\n",
      "         [-0.4176,  0.4257,  0.1715, -0.4700, -0.7427, -0.5625]],\n",
      "\n",
      "        [[ 0.0618, -0.0698, -0.1751, -0.0632, -0.6651, -0.5935],\n",
      "         [-0.2447,  0.3075,  0.3275, -0.3468, -0.9567, -0.9076],\n",
      "         [-0.2248,  0.2904,  0.3332, -0.3266, -0.9397, -0.9034],\n",
      "         [-0.2204,  0.2532,  0.2174, -0.2655, -0.5439, -0.4879],\n",
      "         [ 0.1968, -0.0995,  0.3421,  0.1291, -0.3686, -0.5725],\n",
      "         [-0.4176,  0.4257,  0.1715, -0.4700, -0.7427, -0.5625]]],\n",
      "       grad_fn=<UnsafeViewBackward0>)\n",
      "keys : tensor([[[[ 0.0618, -0.0698, -0.1751],\n",
      "          [-0.0632, -0.6651, -0.5935]],\n",
      "\n",
      "         [[-0.2447,  0.3075,  0.3275],\n",
      "          [-0.3468, -0.9567, -0.9076]],\n",
      "\n",
      "         [[-0.2248,  0.2904,  0.3332],\n",
      "          [-0.3266, -0.9397, -0.9034]],\n",
      "\n",
      "         [[-0.2204,  0.2532,  0.2174],\n",
      "          [-0.2655, -0.5439, -0.4879]],\n",
      "\n",
      "         [[ 0.1968, -0.0995,  0.3421],\n",
      "          [ 0.1291, -0.3686, -0.5725]],\n",
      "\n",
      "         [[-0.4176,  0.4257,  0.1715],\n",
      "          [-0.4700, -0.7427, -0.5625]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0618, -0.0698, -0.1751],\n",
      "          [-0.0632, -0.6651, -0.5935]],\n",
      "\n",
      "         [[-0.2447,  0.3075,  0.3275],\n",
      "          [-0.3468, -0.9567, -0.9076]],\n",
      "\n",
      "         [[-0.2248,  0.2904,  0.3332],\n",
      "          [-0.3266, -0.9397, -0.9034]],\n",
      "\n",
      "         [[-0.2204,  0.2532,  0.2174],\n",
      "          [-0.2655, -0.5439, -0.4879]],\n",
      "\n",
      "         [[ 0.1968, -0.0995,  0.3421],\n",
      "          [ 0.1291, -0.3686, -0.5725]],\n",
      "\n",
      "         [[-0.4176,  0.4257,  0.1715],\n",
      "          [-0.4700, -0.7427, -0.5625]]]], grad_fn=<ViewBackward0>)\n",
      "keys : tensor([[[[ 0.0618, -0.0698, -0.1751],\n",
      "          [-0.2447,  0.3075,  0.3275],\n",
      "          [-0.2248,  0.2904,  0.3332],\n",
      "          [-0.2204,  0.2532,  0.2174],\n",
      "          [ 0.1968, -0.0995,  0.3421],\n",
      "          [-0.4176,  0.4257,  0.1715]],\n",
      "\n",
      "         [[-0.0632, -0.6651, -0.5935],\n",
      "          [-0.3468, -0.9567, -0.9076],\n",
      "          [-0.3266, -0.9397, -0.9034],\n",
      "          [-0.2655, -0.5439, -0.4879],\n",
      "          [ 0.1291, -0.3686, -0.5725],\n",
      "          [-0.4700, -0.7427, -0.5625]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0618, -0.0698, -0.1751],\n",
      "          [-0.2447,  0.3075,  0.3275],\n",
      "          [-0.2248,  0.2904,  0.3332],\n",
      "          [-0.2204,  0.2532,  0.2174],\n",
      "          [ 0.1968, -0.0995,  0.3421],\n",
      "          [-0.4176,  0.4257,  0.1715]],\n",
      "\n",
      "         [[-0.0632, -0.6651, -0.5935],\n",
      "          [-0.3468, -0.9567, -0.9076],\n",
      "          [-0.3266, -0.9397, -0.9034],\n",
      "          [-0.2655, -0.5439, -0.4879],\n",
      "          [ 0.1291, -0.3686, -0.5725],\n",
      "          [-0.4700, -0.7427, -0.5625]]]], grad_fn=<TransposeBackward0>)\n",
      "attn_scores : tensor([[[[ 0.0821,    -inf,    -inf,    -inf,    -inf,    -inf],\n",
      "          [ 0.0219, -0.0205,    -inf,    -inf,    -inf,    -inf],\n",
      "          [ 0.0240, -0.0255, -0.0331,    -inf,    -inf,    -inf],\n",
      "          [-0.0052,  0.0381,  0.0323,  0.0413,    -inf,    -inf],\n",
      "          [ 0.0543, -0.1069, -0.1112, -0.0636, -0.1573,    -inf],\n",
      "          [-0.0234,  0.0823,  0.0756,  0.0744, -0.0656,  0.1404]],\n",
      "\n",
      "         [[-0.2927,    -inf,    -inf,    -inf,    -inf,    -inf],\n",
      "          [-0.2968, -0.4532,    -inf,    -inf,    -inf,    -inf],\n",
      "          [-0.3001, -0.4535, -0.4481,    -inf,    -inf,    -inf],\n",
      "          [-0.1307, -0.2117, -0.2071, -0.1253,    -inf,    -inf],\n",
      "          [-0.2736, -0.3312, -0.3339, -0.1569, -0.2883,    -inf],\n",
      "          [-0.1120, -0.2273, -0.2191, -0.1486, -0.0087, -0.2356]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0821,    -inf,    -inf,    -inf,    -inf,    -inf],\n",
      "          [ 0.0219, -0.0205,    -inf,    -inf,    -inf,    -inf],\n",
      "          [ 0.0240, -0.0255, -0.0331,    -inf,    -inf,    -inf],\n",
      "          [-0.0052,  0.0381,  0.0323,  0.0413,    -inf,    -inf],\n",
      "          [ 0.0543, -0.1069, -0.1112, -0.0636, -0.1573,    -inf],\n",
      "          [-0.0234,  0.0823,  0.0756,  0.0744, -0.0656,  0.1404]],\n",
      "\n",
      "         [[-0.2927,    -inf,    -inf,    -inf,    -inf,    -inf],\n",
      "          [-0.2968, -0.4532,    -inf,    -inf,    -inf,    -inf],\n",
      "          [-0.3001, -0.4535, -0.4481,    -inf,    -inf,    -inf],\n",
      "          [-0.1307, -0.2117, -0.2071, -0.1253,    -inf,    -inf],\n",
      "          [-0.2736, -0.3312, -0.3339, -0.1569, -0.2883,    -inf],\n",
      "          [-0.1120, -0.2273, -0.2191, -0.1486, -0.0087, -0.2356]]]],\n",
      "       grad_fn=<MaskedFillBackward0>)\n",
      "Attn_weights : tensor([[[[1.1111, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [0.5624, 0.5487, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [0.3780, 0.3674, 0.3657, 0.0000, 0.0000, 0.0000],\n",
      "          [0.2727, 0.0000, 0.2787, 0.2801, 0.0000, 0.0000],\n",
      "          [0.2395, 0.2182, 0.2177, 0.2237, 0.2120, 0.0000],\n",
      "          [0.1776, 0.1888, 0.1881, 0.1880, 0.1734, 0.1953]],\n",
      "\n",
      "         [[1.1111, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [0.5806, 0.5305, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [0.3922, 0.3589, 0.3600, 0.0000, 0.0000, 0.0000],\n",
      "          [0.2839, 0.2709, 0.0000, 0.2848, 0.0000, 0.0000],\n",
      "          [0.2225, 0.0000, 0.2149, 0.2380, 0.2206, 0.0000],\n",
      "          [0.1900, 0.1778, 0.1786, 0.1861, 0.2017, 0.1769]]],\n",
      "\n",
      "\n",
      "        [[[1.1111, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [0.0000, 0.5487, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [0.3780, 0.3674, 0.3657, 0.0000, 0.0000, 0.0000],\n",
      "          [0.2727, 0.2796, 0.2787, 0.0000, 0.0000, 0.0000],\n",
      "          [0.2395, 0.2182, 0.0000, 0.2237, 0.2120, 0.0000],\n",
      "          [0.1776, 0.0000, 0.1881, 0.1880, 0.0000, 0.1953]],\n",
      "\n",
      "         [[1.1111, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [0.3922, 0.3589, 0.3600, 0.0000, 0.0000, 0.0000],\n",
      "          [0.2839, 0.2709, 0.2716, 0.2848, 0.0000, 0.0000],\n",
      "          [0.2225, 0.0000, 0.2149, 0.2380, 0.2206, 0.0000],\n",
      "          [0.1900, 0.1778, 0.1786, 0.1861, 0.2017, 0.1769]]]],\n",
      "       grad_fn=<MulBackward0>)\n",
      "attn_weights@values : tensor([[[[-0.1289,  0.0667, -0.8332],\n",
      "          [-0.0021, -0.1889, -0.9166],\n",
      "          [ 0.0375, -0.2789, -0.9428],\n",
      "          [ 0.0303, -0.1649, -0.5836],\n",
      "          [ 0.0304, -0.3237, -0.7929],\n",
      "          [ 0.0684, -0.3019, -0.7614]],\n",
      "\n",
      "         [[ 0.2641, -0.1287, -0.1489],\n",
      "          [ 0.4285, -0.2048, -0.0969],\n",
      "          [ 0.4859, -0.2331, -0.0748],\n",
      "          [ 0.3052, -0.1412, -0.0563],\n",
      "          [ 0.3203, -0.1704, -0.0068],\n",
      "          [ 0.4309, -0.2105, -0.0320]]],\n",
      "\n",
      "\n",
      "        [[[-0.1289,  0.0667, -0.8332],\n",
      "          [ 0.0632, -0.2227, -0.4948],\n",
      "          [ 0.0375, -0.2789, -0.9428],\n",
      "          [ 0.0303, -0.2133, -0.7066],\n",
      "          [ 0.0071, -0.2329, -0.5976],\n",
      "          [ 0.0597, -0.1380, -0.4945]],\n",
      "\n",
      "         [[ 0.2641, -0.1287, -0.1489],\n",
      "          [ 0.0000,  0.0000,  0.0000],\n",
      "          [ 0.4859, -0.2331, -0.0748],\n",
      "          [ 0.4532, -0.2126, -0.0633],\n",
      "          [ 0.3203, -0.1704, -0.0068],\n",
      "          [ 0.4309, -0.2105, -0.0320]]]], grad_fn=<UnsafeViewBackward0>)\n",
      "context_vecs : tensor([[[[-0.1289,  0.0667, -0.8332],\n",
      "          [ 0.2641, -0.1287, -0.1489]],\n",
      "\n",
      "         [[-0.0021, -0.1889, -0.9166],\n",
      "          [ 0.4285, -0.2048, -0.0969]],\n",
      "\n",
      "         [[ 0.0375, -0.2789, -0.9428],\n",
      "          [ 0.4859, -0.2331, -0.0748]],\n",
      "\n",
      "         [[ 0.0303, -0.1649, -0.5836],\n",
      "          [ 0.3052, -0.1412, -0.0563]],\n",
      "\n",
      "         [[ 0.0304, -0.3237, -0.7929],\n",
      "          [ 0.3203, -0.1704, -0.0068]],\n",
      "\n",
      "         [[ 0.0684, -0.3019, -0.7614],\n",
      "          [ 0.4309, -0.2105, -0.0320]]],\n",
      "\n",
      "\n",
      "        [[[-0.1289,  0.0667, -0.8332],\n",
      "          [ 0.2641, -0.1287, -0.1489]],\n",
      "\n",
      "         [[ 0.0632, -0.2227, -0.4948],\n",
      "          [ 0.0000,  0.0000,  0.0000]],\n",
      "\n",
      "         [[ 0.0375, -0.2789, -0.9428],\n",
      "          [ 0.4859, -0.2331, -0.0748]],\n",
      "\n",
      "         [[ 0.0303, -0.2133, -0.7066],\n",
      "          [ 0.4532, -0.2126, -0.0633]],\n",
      "\n",
      "         [[ 0.0071, -0.2329, -0.5976],\n",
      "          [ 0.3203, -0.1704, -0.0068]],\n",
      "\n",
      "         [[ 0.0597, -0.1380, -0.4945],\n",
      "          [ 0.4309, -0.2105, -0.0320]]]], grad_fn=<TransposeBackward0>)\n",
      "context_vecs : tensor([[[-0.1289,  0.0667, -0.8332,  0.2641, -0.1287, -0.1489],\n",
      "         [-0.0021, -0.1889, -0.9166,  0.4285, -0.2048, -0.0969],\n",
      "         [ 0.0375, -0.2789, -0.9428,  0.4859, -0.2331, -0.0748],\n",
      "         [ 0.0303, -0.1649, -0.5836,  0.3052, -0.1412, -0.0563],\n",
      "         [ 0.0304, -0.3237, -0.7929,  0.3203, -0.1704, -0.0068],\n",
      "         [ 0.0684, -0.3019, -0.7614,  0.4309, -0.2105, -0.0320]],\n",
      "\n",
      "        [[-0.1289,  0.0667, -0.8332,  0.2641, -0.1287, -0.1489],\n",
      "         [ 0.0632, -0.2227, -0.4948,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0375, -0.2789, -0.9428,  0.4859, -0.2331, -0.0748],\n",
      "         [ 0.0303, -0.2133, -0.7066,  0.4532, -0.2126, -0.0633],\n",
      "         [ 0.0071, -0.2329, -0.5976,  0.3203, -0.1704, -0.0068],\n",
      "         [ 0.0597, -0.1380, -0.4945,  0.4309, -0.2105, -0.0320]]],\n",
      "       grad_fn=<ViewBackward0>)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.4615,  0.0466, -0.0957, -0.0997,  0.4638, -0.1654],\n",
       "         [ 0.5027,  0.0054, -0.2106, -0.0236,  0.5276, -0.1192],\n",
       "         [ 0.5174, -0.0086, -0.2539,  0.0018,  0.5512, -0.1015],\n",
       "         [ 0.4743, -0.0484, -0.1432, -0.0860,  0.4775, -0.0169],\n",
       "         [ 0.4799, -0.0101, -0.2614, -0.0658,  0.5580, -0.0273],\n",
       "         [ 0.5062, -0.0405, -0.2377, -0.0285,  0.5370, -0.0331]],\n",
       "\n",
       "        [[ 0.4615,  0.0466, -0.0957, -0.0997,  0.4638, -0.1654],\n",
       "         [ 0.4000, -0.0204, -0.1297, -0.1882,  0.4936,  0.0512],\n",
       "         [ 0.5174, -0.0086, -0.2539,  0.0018,  0.5512, -0.1015],\n",
       "         [ 0.5103, -0.0508, -0.1956, -0.0288,  0.5020, -0.0481],\n",
       "         [ 0.4813, -0.0433, -0.2037, -0.0852,  0.5100,  0.0032],\n",
       "         [ 0.5091, -0.0790, -0.1353, -0.0593,  0.4673,  0.0006]]],\n",
       "       grad_fn=<ViewBackward0>)"
      ]
     },
     "execution_count": 214,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mha = MultiHeadAttention(d_in=3, d_out=6, num_heads=2, context_length=6, dropout=0.1)\n",
    "mha(batch)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
