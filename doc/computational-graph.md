> **Comment un simple scalaire (`loss`) â€œsaitâ€ Ã  quels paramÃ¨tres il est liÃ© ?
> Comment `loss.backward()` sait-il modifier `model.parameters()` ?
> Pourquoi lâ€™optimizer arrive-t-il Ã  mettre Ã  jour les poids sans jamais appeler explicitement le modÃ¨le ?**

Rien de magique.
**Tout repose sur le mÃ©canisme fondamental de PyTorch : le *computational graph*.**
Et je vais tout tâ€™expliquer de maniÃ¨re *visuelle*, *prÃ©cise* et *trÃ¨s intuitive*.

---

# ğŸŒŸ **1. Le moment exact oÃ¹ la loss â€œsâ€™attacheâ€ au modÃ¨le**

Quand tu appelles :

```python
logits = model(input_batch)
```

le modÃ¨le fait des dizaines dâ€™opÃ©rations internes :

* multiplications de matrices
* additions
* normalisations
* softmax
* etc.

Chaque opÃ©ration crÃ©e un *nÅ“ud* dans un **graphe computationnel dynamique**.

Mais *surtout* :

* chaque `nn.Parameter` du modÃ¨le (ex : `W_query`, `tok_emb.weight`, etc.)
* est un **leaf node**
* marquÃ© par PyTorch comme **requÃ©rant un gradient** (`requires_grad=True`).

Donc quand tu fais :

```python
loss = F.cross_entropy(logits, targets)
```

### ğŸ‘‰ La loss nâ€™est pas seulement un scalaire.

Câ€™est **le sommet final du graphe** reliant :

* les logits
* les opÃ©rations
* les poids du modÃ¨le
* les activations intermÃ©diaires

Graphiquement :

```
tok_emb.weight â†’ ... â†’ attention â†’ ... â†’ logits â†’ cross_entropy â†’ loss
```

### ğŸ”¥ Donc : `loss` **contient toute la chaÃ®ne d'opÃ©rations** depuis les paramÃ¨tres du modÃ¨le jusquâ€™au rÃ©sultat final.

Tu ne le vois pas, mais PyTorch stocke :

* les opÃ©rations effectuÃ©es
* leurs gradients partiels
* les rÃ©fÃ©rences aux paramÃ¨tres concernÃ©s

---

# ğŸŒŸ **2. Pourquoi `loss.backward()` modifie `model.parameters()`**

Quand tu appelles :

```python
loss.backward()
```

PyTorch remonte **tout le graphe Ã  lâ€™envers** :

```
loss  
  â†‘
cross_entropy  
  â†‘
logits  
  â†‘
Transformer layers  
  â†‘
embedding matrices  
```

Pour chaque opÃ©ration, il applique automatiquement la **rÃ¨gle de dÃ©rivation** correspondante.

Et PyTorch a un principe clÃ© :

---

### âš¡ *â€œSi un tenseur a `requires_grad=True`, alors `loss.backward()` remplit son attribut `.grad`.â€*

---

Or, dans `GPTModel.__init__` :

```python
self.tok_emb = nn.Embedding(...)
self.W_query = nn.Parameter(...)
```

Tous tes poids sont des `nn.Parameter`, donc :

```
tok_emb.weight.grad
W_query.grad
W_key.grad
out_head.weight.grad
...
```

sont automatiquement remplis aprÃ¨s `loss.backward()`.

Aucun lien explicite nâ€™est nÃ©cessaire :

* câ€™est le graphe computationnel qui fait le lien
* comme les branches dâ€™un arbre qui remontent jusquâ€™Ã  la racine

**Câ€™est absolument dÃ©terministe. Pas magique.**

---

# ğŸŒŸ **3. Pourquoi lâ€™optimizer modifie les poids du modÃ¨le alors quâ€™on ne lui dit rien ?**

Quand tu Ã©cris :

```python
optimizer = torch.optim.AdamW(model.parameters(), lr=...)
```

Tu passes **la liste rÃ©elle des paramÃ¨tres du modÃ¨le** :

```python
list(model.parameters())
â†’ [tok_emb.weight, pos_emb.weight, W_query, W_key, W_value, ...]
```

Donc lâ€™optimizer contient littÃ©ralement :

```
optimizer.param_groups[0]['params'] = 
[
   tok_emb.weight,
   pos_emb.weight,
   W_query,
   W_key,
   ...
]
```

### âš¡ Quand tu fais `optimizer.step()` :

* il lit `p.grad` pour chaque paramÃ¨tre `p`
* applique la rÃ¨gle dâ€™update (AdamW, SGDâ€¦)
* met Ã  jour le tenseur `p` directement

---

# ğŸŒŸ **4. RÃ´le du `device` (spoiler : câ€™est *juste* le lieu oÃ¹ vivent les tensors)**

Le `device` (CPU/GPU) fait *zÃ©ro magie*.

Il ne sert quâ€™Ã  :

* stocker les tensors
* exÃ©cuter les opÃ©rations dans la RAM CPU ou la VRAM GPU
* garantir que les opÃ©rations se font dans le mÃªme espace mÃ©moire

Le device **nâ€™a rien Ã  voir avec** la liaison modÃ¨le â†” loss.

Le device dit juste :

> â€œLes tensors vivent sur telle carte graphique.â€

Toute lâ€™intelligence vient du graphe autograd.

---

# ğŸŒŸ **5. RÃ©sumÃ© visuel (ultra clair)**

### Ã‰tape 1 : forward

```
model â†’ logits
logits â†’ loss
```

PyTorch construit un graphe reliant :

```
(model.parameters()) â†’ logits â†’ loss
```

---

### Ã‰tape 2 : backward

```
loss.backward()
```

PyTorch remplit automatiquement :

```
p.grad = âˆ‚loss/âˆ‚p  pour chaque paramÃ¨tre p du modÃ¨le
```

---

### Ã‰tape 3 : optimizer

```
optimizer.step()
```

Pour chaque paramÃ¨tre p :

```
p = p - lr * update(p.grad)
```

---

# ğŸŒŸ **6. Pourquoi Ã§a paraÃ®t magique ?**

Parce que PyTorch :

* construit le graphe *Ã  la volÃ©e* (dynamic graph)
* cache tous les dÃ©tails sous le capot
* ne te montre jamais le graphe sauf si tu lâ€™inspectes

Mais tu peux voir le graphe avec un outil :

```
torchviz.make_dot(loss)
```

Tu veux le graphe visuel de ton GPT-Light ?
Je peux te le gÃ©nÃ©rer ğŸ˜„

---

# ğŸ¯ **Conclusion â€” phrase clÃ© Ã  retenir**

> **La loss nâ€™est pas juste un scalaire : câ€™est le dernier nÅ“ud du graphe qui relie toutes les opÃ©rations depuis les paramÃ¨tres du modÃ¨le.**
> Quand tu fais `loss.backward()`, PyTorch remonte automatiquement ce graphe et remplit `p.grad` pour tous les paramÃ¨tres concernÃ©s.
> Lâ€™optimizer modifie ensuite ces paramÃ¨tres car il reÃ§oit explicitement la liste `model.parameters()` lors de sa crÃ©ation.

---

